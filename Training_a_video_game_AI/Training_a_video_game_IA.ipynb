{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning - Training an artificial intelligence (AI) to play a video game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Artifical intelligence (AI) in video games** is often criticised for its **approximations** or **aberrant behaviours** that break the player's immersion. This AI is traditionally \"hard coded\", so that the AI entity (a NPC - Non-Playable Character - or an enemy for example) will behave according to **predefined rules**, making it very predictable in its actions.\n",
    "\n",
    "In the 1990s, the emergence of **deep learning** (and especially of **increased computational capacity**) found an application in AI through **reinforcement learning**. This branch seeks to **copy the learning mechanism of a human to adapt it to an AI**, making the latter more \"intelligent\" and able to adjust to new situations.\n",
    "\n",
    "The idea? **Confront an AI (usually called an \"agent\") with no knowledge or prior knowledge with a video game**, and **let it learn from its mistakes** in an autonomous way thanks to the **rewards or penalties** that it will be given following its actions and their results.\n",
    "\n",
    "This is essentially what we tried to do in this project, using the **Gym** library, developed by one of the big names in the sector, **OpenAI**, which allowed us to simulate the environment of the game **Activision Tennis**, released in 1980 on the Atari 2600.\n",
    "\n",
    "We will, of course, see the process in detail, but the principle is as follows: our agent's only initial reference point is the **list of actions it can undertake**. First, we give it an **initial image of the game**, which is literally a freeze frame of the latter. The agent is asked to select the **most appropriate action**: move right, left, diagonally, shoot etc. This action is passed on to the game and the game moves on to the next image (called **\"state\"**), which is again passed on to the agent. At the end of the point (as a reminder, the game is a tennis game), the agent is told by the **score** whether it has won or not. \n",
    "\n",
    "**Our agent integrates this experience and draws conclusions from it**: depending on the outcome of the point, it will **modify the probability of choosing a particular action via bonuses/maluses**. Little by little, just like a human would do for a video game or tennis, it will learn from its mistakes and constantly improve, until it reaches a level comparable (or even superior) to that of a human.\n",
    "\n",
    "That's it for the main lines. If you are interested in the subject, the Internet is full of **articles explaining it in much more detail**! \n",
    "\n",
    "Before going any further, we feel it is important to point out that among the many resources we have used, **[Game of Dimension's work on the game Pong](https://github.com/gameofdimension/policy-gradient-pong/blob/master/policy_gradient_pong.py)** stands out, and we have drawn heavily on it.\n",
    "\n",
    "For the French speakers among you, don't hesitate if you want a more \"visual\" introduction to this project to watch the **[replay of the presentation](https://www.youtube.com/watch?v=XefhnowWji4&t=8210s)** that my classmate Chunyan Frey and I made about it.\n",
    "\n",
    "Ready to go?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get to the heart of the matter, let's discuss what you need for this project.\n",
    "\n",
    "As our agent learn from its experiences, there is **no pre-existing dataset**, so all you'll need is the **notebook** provided. However, as explained in the **README file** of this project, please remember that although the code is working, **we haven't been able to complete the training of our agent** for time reasons and are thus unsure of the results.\n",
    "\n",
    "In case you would like to complete the training yourself, **several options** are available: running the notebook on your **own computer** (if the latter is powerful enough), executing the code in a Python script through a **cloud solution** (Amazon SageMaker or Google Cloud Platform for example) or splitting the training in several parts thanks to the **checkpoints**.\n",
    "\n",
    "As for **libraries**, this project uses six of them: \n",
    "\n",
    "- **NumPy** (data manipulation)\n",
    "- **TensorFlow** (data manipulation and deep learning)\n",
    "- **Gym** (video game environments)\n",
    "- **time** (time access) \n",
    "- **os** (operating system interfaces)\n",
    "- **Matplotlib** (data visualization)\n",
    "\n",
    "Depending on your execution environment, you might need to **install these modules**. You can do so using a `pip install name_of_the_library` command. Remember to add an exclamation mark at the beginning if you want to execute it directly in a notebook, as it is originally a terminal command.\n",
    "\n",
    "It is now time to begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Installing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining some useful functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our training is based on a **deep learning architecture**, and we can't feed our model with any input. We could actually, but the images wouldn't be easily understood by the model. Preprocessing the images to simplify the information it contains can thus tremendously reduce training time! Let's do this through a custom function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro(img):\n",
    "    # Cropping the image\n",
    "    img = img[40:220]\n",
    "    \n",
    "    # Reducing the number of pixels by 2 in width and height,\n",
    "    # and removing the last channel\n",
    "    img = img[::2, ::2, 0].astype(np.float)\n",
    "    \n",
    "    # Setting some pixels to specific values\n",
    "    # This is of course not random, we did some tests to see\n",
    "    # which values we had to modify.\n",
    "    img[img == 168] = 0.0\n",
    "    img[img == 214] = 0.5\n",
    "    img[img == 117] = 1.0\n",
    "    img[img == 240] = 1.0\n",
    "    img[img == 74] = 0.0\n",
    "    img[img == 236] = 1.0\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I guess this is not very concrete at the moment. Let's see the **before and after** image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# BEFORE\n",
    "\n",
    "## Creating the game environment and resetting it\n",
    "## (this is mandatory to make it work)\n",
    "env_tennis = gym.make(\"Tennis-v0\")\n",
    "env_tennis.reset()\n",
    "\n",
    "## Simulating 10 frames, with 25 actions chosen randomely\n",
    "## We will explain later on what are these four objects returned by the environment.\n",
    "for _ in range(10):\n",
    "    observation, reward, done, info = env_tennis.step(env_tennis.action_space.sample())\n",
    "\n",
    "## Displaying the new image and closing the environment (this is a good practice)\n",
    "plt.imshow(observation)\n",
    "env_tennis.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1eff0b410d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# AFTER\n",
    "\n",
    "## Calling our custom function on our image\n",
    "after_version = prepro(observation)\n",
    "plt.imshow(after_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can notice, the image has been cropped to focus on its relevant part, and the color code has been simplified in order to **help the algorithm analysing the image**.\n",
    "\n",
    "Another function we have to define upstream is the one establishing the **rewards policy**. This notion of reward is fundamental in reinforcement learning, since **it allows our agent to know if its actions have been successful or not**.\n",
    "\n",
    "In its initial version, the reward is configured as follows: if the agent **wins the point**, we give it a **reward worth 1**. If it **loses the point**, **the reward is worth 0**. However, these rewards are initially given only to the **last action**.\n",
    "\n",
    "This is where the notion of **\"discounted reward\"** comes into play. The idea is to **dilute the reward on all the actions that led to the conclusion** (winning the point or not). In the case of a win, the last action receives a reward of 1 multiplied by a **discount coefficient**, in our case 0.99. For the second to last action, we take this new discounted reward for the last action and multiply it once again by the discount coefficient. The reward is therefore 0.99 * 0.99, that is 0.9801. For the antepenultimate action, it will be 0.970299 (0.9801 * 0.99), and so on.\n",
    "\n",
    "Why apply this discount and not give a reward of 1 for all actions that led to the agent winning the point? Simply because we consider that **the closer an action is to the end of the point, the more decisive it was**. If our agent has won the point, it is likely that his last moves were good, and should be encouraged.\n",
    "\n",
    "Conversely, when our agent **loses the point**, the reward will be **-1**, and the same discount process is applied.\n",
    "\n",
    "This system is not perfect, but in general, and by **repetition of experiences**, **the relevant actions will tend to be encouraged, and the bad actions penalized**. This is how our agent will progress!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our reward matrix is initially composed of 0 (most of the times),\n",
    "# -1 or 1 (for the last actions of the points won).\n",
    "# The function will replace it by its discounted version.\n",
    "\n",
    "def discount_rewards(r):\n",
    "    # Our discount coefficient\n",
    "    gamma = 0.99\n",
    "    \n",
    "    # A \"placeholder\" matrix, similar in shape to the matrix passed to the function but filled with 0\n",
    "    discounted_r = np.zeros_like(r) \n",
    "    \n",
    "    # The last reward granted, initialized at 0\n",
    "    running_add = 0\n",
    "    \n",
    "    # Looping backwards over our reward matrix, starting from its last element\n",
    "    for t in reversed(range(0, len(r))):\n",
    "        # Checking if the selected element is different from 0 \n",
    "        # If it is the case, we replace the current state of the reward by the element (1 or -1).\n",
    "        # As our matrix contains rewards for several points, this will allow \n",
    "        # us to reinitialize the discounting process every time we encounter\n",
    "        # a 1 or -1 in the original matrix, which indicates that this is a different point.\n",
    "        if r[t] != 0: running_add = r[t]\n",
    "        \n",
    "        # Applying our discount coefficient to the current state of the reward\n",
    "        running_add = running_add * gamma\n",
    "        \n",
    "        # Replacing the element in our placeholder matrix by the calculated value\n",
    "        discounted_r[t] = running_add\n",
    "        \n",
    "    # Once the loop is over, we want to return the discounted matrix.    \n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last function that we will have to define before entering the heart of the matter refers to the **architecture of the model that we will use to select the actions to be performed at each step**. This model is based on **deep learning** and is quite simple since it is composed of **two dense layers**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_network(pixels_num, hidden_units, action_number):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(hidden_units, activation = \"relu\", input_shape = [pixels_num]),\n",
    "        \n",
    "        # We want to output a probability for each possible action, so our last layer\n",
    "        # contains a number of neurons equals to the number of possible actions.\n",
    "        tf.keras.layers.Dense(action_number, activation = \"softmax\")\n",
    "        ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setting the values of some variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last step before starting the training: we need to **define some variables and parameters**. This is not systematically necessary, but **it simplifies the training loop** and makes the **different parameters easily adjustable** for other experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of pixels of our images after transformation\n",
    "pixels_num = after_version.shape[0] * after_version.shape[1]\n",
    "\n",
    "# Number of neurons in the first dense layer of our model\n",
    "hidden_units = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need the **number of actions** our agent can make in the game, as this will determine the **number of neurons of the last layer of our model**. We can't retrieve this number directly due to the way the library is coded, but we can display it and save it in a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(18)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing the simulation of our game\n",
    "# Note: you can easily adapt this notebook to another game, feel free to visit OpenAI Gym's \n",
    "# documentation (https://gym.openai.com/envs) to see which games are available!\n",
    "env = gym.make(\"Tennis-v0\")\n",
    "\n",
    "# Displaying the number of available actions\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the displayed number in a variable\n",
    "\n",
    "action_number = 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initializing our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating our deep learning model, using the function defined earlier\n",
    "model = make_network(pixels_num, hidden_units, action_number)\n",
    "\n",
    "# Instantiating an Adam optimizer with a learning rate of 0.001\n",
    "model_optimizer = tf.keras.optimizers.Adam(lr = 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to move on to the longest and most complex phase of the project: the **training of the agent**. Some cells, such as the training loop itself, will be particularly dense, but I'll go through it **step by step**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reloading from the last checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentionned at the beginning of this notebook, **training an AI agent can be very long**. Hence the utility of **configuring checkpoints**! The latter will be done directly in the training loop, all we have to do for now is **deciding whether or not we want to resume from a previous save**, and loading the corresponding elements if the answer is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If it's the first time you launch the training, the `resume` variable must be set to False. \n",
    "# If you want to resume from a checkpoint, all you need to change in the whole notebook is\n",
    "# setting it to True! And verifying you have all the required files in the right folder, obviously.\n",
    "resume = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Managing the checkpoints' directory\n",
    "\n",
    "## Defining the desired relative path of our directory\n",
    "checkpoint_dir = \"./checkdir\"\n",
    "\n",
    "## Creating the directory if it doesn't exist yet\n",
    "if not(os.path.isdir(checkpoint_dir)): os.mkdir(checkpoint_dir)\n",
    "    \n",
    "## Setting 'ckpt' as the prefix of each save file \n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "\n",
    "## Making the checkpoint process ready for use\n",
    "checkpoint = tf.train.Checkpoint(model = model, optimizer = model_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing several commands if 'resume' has been set to True\n",
    "\n",
    "if resume:\n",
    "    ## Loading the latest save file\n",
    "    latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    checkpoint.restore(latest)\n",
    "    \n",
    "    ## Getting the corresponding model and optimizer\n",
    "    model = checkpoint.model\n",
    "    model_optimizer = checkpoint.optimizer\n",
    "    \n",
    "    ## Looking for the training step number (more on this further on) in the name of the checkpoint\n",
    "    start = latest.find(\"ckpt-\") + 5\n",
    "    last_step = int(latest[start:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initializing a few variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before launching the training loop, we have (once again) to **initialize some variables and objects**!\n",
    "\n",
    "But first, we have to come back to something. Remember earlier, when I said that I would explain the line `observation, reward, done, info = env_tennis.step(env_tennis.action_space.sample())`? It's time now, as this will be required to understand the following cells.\n",
    "\n",
    "Let's first simplify this command, to make it clearer. `env_tennis.action_space.sample()` just gives us a **random number** between 0 and 17, which corresponds to **one of the available actions in the game** (move right, shoot, move back etc.). To understand the whole line, we don't care about the action taken, so we'll just replace this bit by an integer (for example 4).\n",
    "\n",
    "A **simplified version** of the command would then be `observation, reward, done, info = env_tennis.step(4)`. Easier to read, but what does this mean? Let's first focus on `env_tennis.step(4)`. This means that we take the **current state of our environment (our simulation) and make it evolve through an action** (here action number 4). This `step` function will return four elements, that we store in **four variables** located on the left part of the command:\n",
    "- `observation`: the **next image**. Once our action is made, the environment will move to **another state** and thus a **new observation**! Once again, picture it as a game that would be displayed **frame by frame**. At each frame, our agent decides which action to implement. We move on to the next frame, and so on.\n",
    "- `reward`: the **reward** associated with the action taken. As mentionned before, **this will be 0 in most cases**, except when the point is done (it will then be 1 or -1 depending on the result - but this only happens once in a while!).\n",
    "- `done`: taking the value True or False, this variable indicates **whether or not the game is finished**. This will be False most of the times, except on the last frame.\n",
    "- `info`: gives more information on the state of the environment. We won't use this.\n",
    "\n",
    "One last thing I have to say concerns the **length of an episode**. The latter corresponds to the duration of a **match**, which in Activision Tennis lasts **one set**. It is not essential to know this, but for those of you who are not familiar with tennis, a set is simply a **collection of games** (four points or more), played until a player wins six games or more. \n",
    "\n",
    "To summarize, in the rest of this notebook, the terms **\"episode\"** and **\"match\"** will be **strictly equivalent**.\n",
    "\n",
    "OK, now most of the next cells will make much more sense to you! Not everything though, I'll elaborate on some points afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicating that when we start an episode, there is no previous observation\n",
    "prev_x = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the batch size, a.k.a. the number of episodes before updating the weights of the model\n",
    "# our agent uses to select the best actions to implement at each observation.\n",
    "# This refers to the very foundations of deep learning: the model is optimized over the course \n",
    "# of iterations by updating the weights of each neuron. \n",
    "# Here, we will ask our agent to play 10 matches, and then we will update \n",
    "# the weights of the model to integrate what it has learned on this interval.\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating several empty lists that will contain respectively:\n",
    "# - all the observations of a batch (x_ba)\n",
    "# - the indexes of all the actions taken in an batch (act_ba)\n",
    "# - all the rewards obtained by our agent during an episode (not the whole batch!), before discount(rew_ep)\n",
    "# - all the rewards obtained by our agent during the batch, after discount(dis_rew_ba)\n",
    "x_ba = []\n",
    "act_ba = []\n",
    "rew_ep = []\n",
    "dis_rew_ba = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicating the current training step (loading it from the checkpoint files or \n",
    "# setting it to 0), as well as the current episode number. This part might still \n",
    "# seem obscure, but I'll get back to it. For know, just remember that a step\n",
    "# is composed of 10 episodes.\n",
    "step = last_step if resume else 0\n",
    "episode_number = step * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the video recorder, which will help us assess the evolution\n",
    "# of our agent! We specify as arguments of the function what we want to record,\n",
    "# where we want to save the results and when to record.\n",
    "# Here we chose to record a video every 500 episodes, in order to to avoid \n",
    "# using too much memory for storage. Remember that the training will last for \n",
    "# days, so you don't want to overload your storage space!\n",
    "env = gym.wrappers.Monitor(env, \n",
    "                           \"recording/\", \n",
    "                           video_callable = lambda episode_number: episode_number % 500 == 0, \n",
    "                           force = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting the environnement before starting\n",
    "observation = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stating that the initial reward mean equals to -24.\n",
    "# This represents the worst result for our agent. At first, it is inexperienced and is likely to lose \n",
    "# all the points played. Given that a player needs to win 6 games, themselves composed of at least \n",
    "# 4 points, to win a match, we can expect our agent to receive a total reward of 6 * 4 * -1 = -24.\n",
    "# Our objective will be to improve this score: our agent will start to win more and more points,\n",
    "# so we can expect its score to increase. The perfect score would be +24, but we don't\n",
    "# need such a high level of performance.\n",
    "reward_mean = -24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Launching (finally!) the training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it, we can finally **start the training loop**! \n",
    "\n",
    "The following is probably the **most complicated part** of the project, but rest assured that it is the last.\n",
    "\n",
    "Also, what we have already discussed combined with the upcoming code comments should allow you to understand it all without too much trouble.\n",
    "\n",
    "By the way, don't worry about the huge **error message** in the output, it's just because I **manually stopped the execution of the cell**. I just wanted to give you an example of what you'll get, but this is far from being the whole training.\n",
    "\n",
    "Otherwise, if you get **outliers** regarding the **frame size** (typically 10000 for several iterations, which is the cutoff value), consider maybe **restarting the training**. Indeed, this may mean that the agent has gone in a **wrong direction of learning** from the beginning, which will be difficult (and especially very long) to rectify.\n",
    "\n",
    "Also, don't be alarmed if **`reward_mean` fluctuates up and down**, this is perfectly **normal in the short term**. It will have an **upward trend in the long run**.\n",
    "\n",
    "Ready to dive in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:46:41, 10 Jun 2021 - episode: 50, epoch: 5, frame size: 23297, reward mean: -23.919810302364787\n",
      "13:52:01, 10 Jun 2021 - episode: 100, epoch: 10, frame size: 24322, reward mean: -23.928444191289664\n",
      "13:57:28, 10 Jun 2021 - episode: 150, epoch: 15, frame size: 24609, reward mean: -23.93317919703471\n",
      "14:03:09, 10 Jun 2021 - episode: 200, epoch: 20, frame size: 23604, reward mean: -23.913558671443646\n",
      "14:09:20, 10 Jun 2021 - episode: 250, epoch: 25, frame size: 23597, reward mean: -23.92666234368268\n",
      "14:15:32, 10 Jun 2021 - episode: 300, epoch: 30, frame size: 23943, reward mean: -23.92075328202214\n",
      "14:21:42, 10 Jun 2021 - episode: 350, epoch: 35, frame size: 23893, reward mean: -23.91659337097026\n",
      "14:28:02, 10 Jun 2021 - episode: 400, epoch: 40, frame size: 24629, reward mean: -23.902563522623367\n",
      "14:34:17, 10 Jun 2021 - episode: 450, epoch: 45, frame size: 24141, reward mean: -23.9165167163315\n",
      "14:40:35, 10 Jun 2021 - episode: 500, epoch: 50, frame size: 24435, reward mean: -23.942242303516423\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-ea0612654b54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;31m# and move to t+1. We recover some objects, including the next observation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m# and the reward our agent got on the current observation.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;31m# Adding the current observation, the action index and the reward obtained\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\wrappers\\monitor.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_before_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_after_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[0mreward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[0mob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame_over\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"ale.lives\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlives\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36m_get_obs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_ram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_obs_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'image'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36m_get_image\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetScreenRGB2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_ram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\atari_py\\ale_python_interface.py\u001b[0m in \u001b[0;36mgetScreenRGB2\u001b[1;34m(self, screen_data)\u001b[0m\n\u001b[0;32m    264\u001b[0m             \u001b[0mscreen_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mscreen_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrides\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m480\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m         \u001b[0male_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetScreenRGB2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ctypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscreen_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mscreen_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# We're using a while loop here, making it run indefinitely\n",
    "# (not really actually, as we will introduce a break condition later on)\n",
    "while True:\n",
    "    # Applying our preprocessing function to the observation\n",
    "    cur_x = prepro(observation)\n",
    "    \n",
    "    # A technical detail: the model doesn't really take into account an observation \n",
    "    # but the difference between the observation at time t and that at t-1.\n",
    "    # If we are at the beginning of a match, there is no observation at t-1,\n",
    "    # so we consider that the latter is just a blank image.\n",
    "    x = cur_x - prev_x if prev_x is not None else np.zeros((pixels_num))\n",
    "    \n",
    "    # Setting the current observation as the one that should be used \n",
    "    # as the previous one in the next iteration. \n",
    "    prev_x = cur_x\n",
    "    \n",
    "    # Deleting objects that are no longer needed, in order to \n",
    "    # unclutter the memory (we will do this a couple of times) \n",
    "    del observation\n",
    "    del cur_x\n",
    "\n",
    "    # Reshaping the observation to make it understandable by the model\n",
    "    x = tf.reshape(x, shape = [1, -1])\n",
    "\n",
    "    # Using the model to select the best possible action given the current situation\n",
    "    # What we get thanks to the model is a matrix of probabilities, which represent\n",
    "    # roughly the chance of each action to be the most relevant.\n",
    "    # We then use Numpy's argmax function to get the index of the action with the highest probability.\n",
    "    tf_probs = model(x)\n",
    "    action_index = np.argmax(tf_probs)\n",
    "    del tf_probs\n",
    "\n",
    "    # Making the environment move to the next state. We talked about it before,\n",
    "    # but to make it short, we ask the environment to apply the action selected\n",
    "    # and move to t+1. We recover some objects, including the next observation\n",
    "    # and the reward our agent got on the current observation.\n",
    "    observation, reward, done, _ = env.step(action_index)\n",
    "    \n",
    "    # Adding the current observation, the action index and the reward obtained\n",
    "    # to the lists we created previously.\n",
    "    x_ba.append(x)\n",
    "    act_ba.append(action_index)\n",
    "    rew_ep.append(reward)\n",
    "\n",
    "    # All the previous operations will be repeated for each observation of an episode (and then of a batch).\n",
    "    # Let's now see what happens when the environment returns the value \"True\" for \n",
    "    # the 'done' object, i.e. when the episode is over.\n",
    "    if done:\n",
    "        # Adding 1 to the episode count\n",
    "        episode_number += 1\n",
    "        \n",
    "        # Replacing the matrix of rewards of the episode by its discounted version.\n",
    "        # We also normalize the matrix, and we add the result to the list we created earlier.\n",
    "        discounted_ep_rew = discount_rewards(rew_ep)\n",
    "        discounted_ep_rew -= np.mean(discounted_ep_rew)\n",
    "        discounted_ep_rew /= np.std(discounted_ep_rew)\n",
    "        dis_rew_ba += discounted_ep_rew.tolist()\n",
    "        del discounted_ep_rew\n",
    "\n",
    "        # Updating the reward mean (which is -24 in the beginning) with the score of the episode.\n",
    "        # We give the latter a small importance in order to make the results more consistant.\n",
    "        # In other words, we make the 'reward_mean' object less sensitive to variations \n",
    "        # and therefore more stable and relevant in the long run.\n",
    "        reward_mean = 0.99 * reward_mean + (1 - 0.99) * sum(rew_ep)\n",
    "\n",
    "        # Reinitializing the episode reward list for the next iteration\n",
    "        rew_ep = []\n",
    "        \n",
    "        # In order to limit training time, we considered that the whole loop should \n",
    "        # stop if 'reward_mean' exceeds 5, as this means our agent is good enough. \n",
    "        # Believe me, I played the game and beating the game's AI is already a great achievement!\n",
    "        if reward_mean > 5.0:\n",
    "            break\n",
    "\n",
    "        # Each time we reach the end of a batch (10 episodes in our case), we modify our model \n",
    "        # to integrate what the agent has learned during the batch.\n",
    "        # This is the heart of the learning process!\n",
    "        if episode_number % batch_size == 0:\n",
    "            # Adding 1 to the training step count\n",
    "            step += 1\n",
    "            \n",
    "            # Determining the number of observations in the batch\n",
    "            frame_size = len(x_ba)\n",
    "            \n",
    "            # Transforming our matrixes of observations, actions and rewards to \n",
    "            # make them understandable by TensorFlow.\n",
    "            x_ba = np.vstack(x_ba)\n",
    "            act_ba = np.vstack(act_ba)\n",
    "            dis_rew_ba = np.vstack(dis_rew_ba)\n",
    "            \n",
    "            # To ease things, we'll collect information from \n",
    "            # our batch by bits of 20,000 observations.\n",
    "            # 'pos' will refer to the latest position in our partition.\n",
    "            stride = 20000\n",
    "            pos = 0\n",
    "\n",
    "            # Infinite loop that will only break when we reach the end of the batch\n",
    "            while True:\n",
    "                # Setting the end of the current bit at 20,000 (or less if we're at the end of the batch)\n",
    "                end = pos + stride if pos + stride < frame_size else frame_size\n",
    "\n",
    "                # Selecting the observations, actions and rewards of the bit\n",
    "                x_bit = x_ba[pos:end]\n",
    "                act_bit = act_ba[pos:end]\n",
    "                rew_bit = dis_rew_ba[pos:end]\n",
    "\n",
    "                # Time to update the model with this new information through a gradient descent,\n",
    "                # with the help of TensorFlow's GradientTape function.\n",
    "                with tf.GradientTape() as tape:\n",
    "                    # Calculating the cross entropy using as true values the actions taken\n",
    "                    # (this is a huge difference from classical machine learning, as we have\n",
    "                    # no preexisting data) and as predicted values what the model predicts\n",
    "                    # for the same observation.\n",
    "                    cross_entropy = tf.losses.sparse_categorical_crossentropy(act_bit, \n",
    "                                                                              model(tf.reshape(x_bit, shape = [len(x_bit), pixels_num])))\n",
    "                    \n",
    "                    # Calculating the loss by comparing the rewards and the cross entropy\n",
    "                    loss = tf.reduce_sum(tf.multiply(rew_bit, cross_entropy))\n",
    "                    \n",
    "                    # Compiling everything...\n",
    "                    gradient = tape.gradient(loss, model.trainable_variables)\n",
    "                    \n",
    "                    # ... and updating our model's weights!\n",
    "                    model_optimizer.apply_gradients(zip(gradient, model.trainable_variables))\n",
    "                \n",
    "                # Indicating that the next iteration will have to start\n",
    "                # where the previous one ended\n",
    "                pos = end\n",
    "\n",
    "                # Breaking the loop if reaching the end of the batch\n",
    "                if pos >= frame_size:\n",
    "                    break\n",
    "            \n",
    "            # Once all the batch has been processed, we prepare the ground for the next batch!\n",
    "            del x_bit\n",
    "            del act_bit\n",
    "            del rew_bit\n",
    "            x_ba = []\n",
    "            act_ba = []\n",
    "            dis_rew_ba = []\n",
    "                \n",
    "        # Every 5 batches, we will save the current state of the model.\n",
    "        if episode_number % (batch_size * 5) == 0:\n",
    "            # Saving the elements according to what was initially set up\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "            # Displaying information to help us track the progress\n",
    "            print(\"{} - episode: {}, epoch: {}, frame size: {}, reward mean: {}\".\\\n",
    "                  format(time.strftime(\"%H:%M:%S, %d %b %Y\"), episode_number, step, frame_size, reward_mean))\n",
    "            \n",
    "        # Resetting the environment in order to start a new episode\n",
    "        observation = env.reset()\n",
    "\n",
    "# Closing the environment if our whole loop stops (i.e. if 'reward_mean' becomes superior to 5)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After **several days of practice**, your agent should reach a sufficient level to consider that **it has learned to play the game**.\n",
    "\n",
    "Although the latter is quite simple and the actions are relatively few (which allowed us to use a simple model), **the training is still very long compared to what we see in machine learning**.\n",
    "\n",
    "So I let you imagine what it could be like to train an agent on a real-time strategy game for example!\n",
    "\n",
    "This brings us to one of the **limits of reinforcement learning**: it's a field that requires a lot of **time** and **large computing capacities**. Behind all the large-scale projects in this field, which have received the most media coverage, are dozens of people and computers.\n",
    "\n",
    "This isn't the case here, but reinforcement learning is typically the kind of problem that benefits from the contributions of **distributed computing**.\n",
    "\n",
    "Ultimately, it's the **rules of the game** and the **range of possible actions** that will determine the complexity of training an agent.\n",
    "\n",
    "This project is just one practical case of reinforcement learning among others. It is possible to explore **many other horizons**, whether it be in terms of the basic material (OpenAI offers environments for [many other games](https://gym.openai.com/envs)), the deep learning model, the learning method etc. Have fun!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
