{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Gn-iFTzixh-"
   },
   "source": [
    "# Trip planning - Prepare your next holiday!\n",
    "\n",
    "## Part 2 - Finding the best hotels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the **rankings of the best French cities to visit in the coming week** (see Part 1), we can move on to **finding hotels to stay at**!\n",
    "\n",
    "To do so, we will select the five best destinations and **scrap [Booking](https://www.booking.com/index.fr.html)'s website** to get the list of hotels there. In addition to the names, the idea will of course be to get **useful information to help us decide which hotel to pick**, such as ratings, locations or descriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting ready for web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nimes', 'Aigues Mortes', 'Aix en Provence', 'Avignon', 'Uzes']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the CSV created previously \n",
    "\n",
    "dataset = pd.read_csv(\"destinations.csv\")\n",
    "\n",
    "# Extracting and displaying the 5 best locations\n",
    "\n",
    "top_destinations = dataset[dataset[\"rank\"] <= 5].loc[:, \"city_name\"].to_list()\n",
    "top_destinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping the list of hotels in our top 5 destinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library we will be using for the web scraping part is **Scrapy**. A minor inconvenience of this library is that we'll have to **restart Jupyter's kernel** each time we move to a new city. First of all, we'll have to **create a class** configuring our \"**spider**\": it will contain all the steps required for scraping a given URL.\n",
    "\n",
    "Then, we'll be able to run our spider for each of our 5 selected destinations and save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a folder that will contain our files\n",
    "\n",
    "os.mkdir(\"hotels_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a class that inherits from Scrapy's Spider class\n",
    "\n",
    "class Hotels(scrapy.Spider):\n",
    "    # Name of the spider\n",
    "    name = \"hotels\"\n",
    "\n",
    "    # Starting URL, i.e. the welcome page of the website\n",
    "    start_urls = [\"https://www.booking.com/index.fr.html\"]\n",
    "\n",
    "    # Simulating a request in the search bar through a parse function\n",
    "    def parse(self, response):\n",
    "        return scrapy.FormRequest.from_response(response,\n",
    "                                                formdata = {\"ss\": destination_name},\n",
    "                                                callback = self.after_search)\n",
    "\n",
    "    # Indicating what to do once search results come up\n",
    "    def after_search(self, response):\n",
    "        # Selecting a specific block in the page through CSS elements\n",
    "        hotels = response.css(\".sr_item\")\n",
    "\n",
    "        # Getting the name, URL, coordinates, score and description of the hotels,\n",
    "        # again through the use of CSS selectors\n",
    "        for h in hotels:\n",
    "            yield {\"hotel_name\": h.css(\".sr-hotel__name::text\").get(),\n",
    "                   \"url\": \"https://www.booking.com\" + h.css(\".hotel_name_link\").attrib[\"href\"],\n",
    "                   \"coords\": h.css(\".sr_card_address_line a\").attrib[\"data-coords\"],\n",
    "                   \"score\": h.css(\".bui-review-score__badge::text\").get(),\n",
    "                   \"description\": h.css(\".hotel_desc::text\").get()}\n",
    "        \n",
    "        # Accessing the next page, if relevant, and reexecuting this function\n",
    "        try:\n",
    "            next_page = response.css(\"a.paging-next\").attrib[\"href\"]\n",
    "        except KeyError:\n",
    "            logging.info(\"No next page. Terminating crawling process.\")\n",
    "        else:\n",
    "            yield response.follow(next_page, callback = self.after_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how to run a spider for one of our destinations. We'll then just have to restart the kernel and move on to the next one! \n",
    "\n",
    "We could save what follows in a function to make our code **DRY (Don't Repeat Yourself)**, but we're facing an exception here: when our kernel restart, any function we would have coded will be **deleted from the memory**, so using such objects is of no use here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the first city of our top 5 as the selected city\n",
    "\n",
    "destination_name = top_destinations[0]\n",
    "\n",
    "# Creating the name of the JSON file that will store the results\n",
    "\n",
    "filename = \"hotels_\" + destination_name.replace(\" \", \"-\") + \".json\"\n",
    "\n",
    "# Checking is a file with this name already exists, and overwriting it if it is\n",
    "# This will only activate if you rerun a code that you already ran\n",
    "\n",
    "if filename in os.listdir(\"hotels_data/\"):\n",
    "    os.remove(\"hotels_data/\" + filename)\n",
    "\n",
    "# Configuring the crawler with the navigator we want to simulate, the amount \n",
    "# of information we want to display and the method used to save the results\n",
    "    \n",
    "process = CrawlerProcess(settings = {\"USER_AGENT\": \"Chrome/84.0 (compatible; MSIE 7.0; Windows NT 5.1)\",\n",
    "                                     \"LOG_LEVEL\": logging.WARNING,\n",
    "                                     \"FEEDS\": {\"hotels_data/\" + filename: {\"format\": \"json\"}}})\n",
    "\n",
    "# Starting the crawler\n",
    "\n",
    "process.crawl(Hotels)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked as intended! Now, you'll have to **restart the kernel** for each other city you want to make an hotel research for.\n",
    "\n",
    "In the following cells, I will **copy-paste everything we wrote so far** in this notebook, so that you'll only have one cell to execute after restarting the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMEMBER TO RESTART YOUR KERNEL BEFORE EXECUTING THIS CELL\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "dataset = pd.read_csv(\"destinations.csv\")\n",
    "top_destinations = dataset[dataset[\"rank\"] <= 5].loc[:, \"city_name\"].to_list()\n",
    "\n",
    "class Hotels(scrapy.Spider):\n",
    "    name = \"hotels\"\n",
    "    start_urls = [\"https://www.booking.com/index.fr.html\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        return scrapy.FormRequest.from_response(response,\n",
    "                                                formdata = {\"ss\": destination_name},\n",
    "                                                callback = self.after_search)\n",
    "\n",
    "    def after_search(self, response):\n",
    "        hotels = response.css(\".sr_item\")\n",
    "        for h in hotels:\n",
    "            yield {\"hotel_name\": h.css(\".sr-hotel__name::text\").get(),\n",
    "                   \"url\": \"https://www.booking.com\" + h.css(\".hotel_name_link\").attrib[\"href\"],\n",
    "                   \"coords\": h.css(\".sr_card_address_line a\").attrib[\"data-coords\"],\n",
    "                   \"score\": h.css(\".bui-review-score__badge::text\").get(),\n",
    "                   \"description\": h.css(\".hotel_desc::text\").get()}\n",
    "        \n",
    "        try:\n",
    "            next_page = response.css(\"a.paging-next\").attrib[\"href\"]\n",
    "        except KeyError:\n",
    "            logging.info(\"No next page. Terminating crawling process.\")\n",
    "        else:\n",
    "            yield response.follow(next_page, callback = self.after_search)\n",
    "            \n",
    "# Here is the only line that changes!\n",
    "\n",
    "destination_name = top_destinations[1]\n",
    "\n",
    "filename = \"hotels_\" + destination_name.replace(\" \", \"-\") + \".json\"\n",
    "\n",
    "if filename in os.listdir(\"hotels_data/\"):\n",
    "    os.remove(\"hotels_data/\" + filename)\n",
    "\n",
    "process = CrawlerProcess(settings = {\"USER_AGENT\": \"Chrome/84.0 (compatible; MSIE 7.0; Windows NT 5.1)\",\n",
    "                                     \"LOG_LEVEL\": logging.WARNING,\n",
    "                                     \"FEEDS\": {\"hotels_data/\" + filename: {\"format\": \"json\"}}})\n",
    "\n",
    "process.crawl(Hotels)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMEMBER TO RESTART YOUR KERNEL BEFORE EXECUTING THIS CELL\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "dataset = pd.read_csv(\"destinations.csv\")\n",
    "top_destinations = dataset[dataset[\"rank\"] <= 5].loc[:, \"city_name\"].to_list()\n",
    "\n",
    "class Hotels(scrapy.Spider):\n",
    "    name = \"hotels\"\n",
    "    start_urls = [\"https://www.booking.com/index.fr.html\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        return scrapy.FormRequest.from_response(response,\n",
    "                                                formdata = {\"ss\": destination_name},\n",
    "                                                callback = self.after_search)\n",
    "\n",
    "    def after_search(self, response):\n",
    "        hotels = response.css(\".sr_item\")\n",
    "        for h in hotels:\n",
    "            yield {\"hotel_name\": h.css(\".sr-hotel__name::text\").get(),\n",
    "                   \"url\": \"https://www.booking.com\" + h.css(\".hotel_name_link\").attrib[\"href\"],\n",
    "                   \"coords\": h.css(\".sr_card_address_line a\").attrib[\"data-coords\"],\n",
    "                   \"score\": h.css(\".bui-review-score__badge::text\").get(),\n",
    "                   \"description\": h.css(\".hotel_desc::text\").get()}\n",
    "        \n",
    "        try:\n",
    "            next_page = response.css(\"a.paging-next\").attrib[\"href\"]\n",
    "        except KeyError:\n",
    "            logging.info(\"No next page. Terminating crawling process.\")\n",
    "        else:\n",
    "            yield response.follow(next_page, callback = self.after_search)\n",
    "            \n",
    "# Here is the only line that changes!\n",
    "\n",
    "destination_name = top_destinations[2]\n",
    "\n",
    "filename = \"hotels_\" + destination_name.replace(\" \", \"-\") + \".json\"\n",
    "\n",
    "if filename in os.listdir(\"hotels_data/\"):\n",
    "    os.remove(\"hotels_data/\" + filename)\n",
    "\n",
    "process = CrawlerProcess(settings = {\"USER_AGENT\": \"Chrome/84.0 (compatible; MSIE 7.0; Windows NT 5.1)\",\n",
    "                                     \"LOG_LEVEL\": logging.WARNING,\n",
    "                                     \"FEEDS\": {\"hotels_data/\" + filename: {\"format\": \"json\"}}})\n",
    "\n",
    "process.crawl(Hotels)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fourth city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMEMBER TO RESTART YOUR KERNEL BEFORE EXECUTING THIS CELL\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "dataset = pd.read_csv(\"destinations.csv\")\n",
    "top_destinations = dataset[dataset[\"rank\"] <= 5].loc[:, \"city_name\"].to_list()\n",
    "\n",
    "class Hotels(scrapy.Spider):\n",
    "    name = \"hotels\"\n",
    "    start_urls = [\"https://www.booking.com/index.fr.html\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        return scrapy.FormRequest.from_response(response,\n",
    "                                                formdata = {\"ss\": destination_name},\n",
    "                                                callback = self.after_search)\n",
    "\n",
    "    def after_search(self, response):\n",
    "        hotels = response.css(\".sr_item\")\n",
    "        for h in hotels:\n",
    "            yield {\"hotel_name\": h.css(\".sr-hotel__name::text\").get(),\n",
    "                   \"url\": \"https://www.booking.com\" + h.css(\".hotel_name_link\").attrib[\"href\"],\n",
    "                   \"coords\": h.css(\".sr_card_address_line a\").attrib[\"data-coords\"],\n",
    "                   \"score\": h.css(\".bui-review-score__badge::text\").get(),\n",
    "                   \"description\": h.css(\".hotel_desc::text\").get()}\n",
    "        \n",
    "        try:\n",
    "            next_page = response.css(\"a.paging-next\").attrib[\"href\"]\n",
    "        except KeyError:\n",
    "            logging.info(\"No next page. Terminating crawling process.\")\n",
    "        else:\n",
    "            yield response.follow(next_page, callback = self.after_search)\n",
    "            \n",
    "# Here is the only line that changes!\n",
    "\n",
    "destination_name = top_destinations[3]\n",
    "\n",
    "filename = \"hotels_\" + destination_name.replace(\" \", \"-\") + \".json\"\n",
    "\n",
    "if filename in os.listdir(\"hotels_data/\"):\n",
    "    os.remove(\"hotels_data/\" + filename)\n",
    "\n",
    "process = CrawlerProcess(settings = {\"USER_AGENT\": \"Chrome/84.0 (compatible; MSIE 7.0; Windows NT 5.1)\",\n",
    "                                     \"LOG_LEVEL\": logging.WARNING,\n",
    "                                     \"FEEDS\": {\"hotels_data/\" + filename: {\"format\": \"json\"}}})\n",
    "\n",
    "process.crawl(Hotels)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fifth city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMEMBER TO RESTART YOUR KERNEL BEFORE EXECUTING THIS CELL\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "dataset = pd.read_csv(\"destinations.csv\")\n",
    "top_destinations = dataset[dataset[\"rank\"] <= 5].loc[:, \"city_name\"].to_list()\n",
    "\n",
    "class Hotels(scrapy.Spider):\n",
    "    name = \"hotels\"\n",
    "    start_urls = [\"https://www.booking.com/index.fr.html\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        return scrapy.FormRequest.from_response(response,\n",
    "                                                formdata = {\"ss\": destination_name},\n",
    "                                                callback = self.after_search)\n",
    "\n",
    "    def after_search(self, response):\n",
    "        hotels = response.css(\".sr_item\")\n",
    "        for h in hotels:\n",
    "            yield {\"hotel_name\": h.css(\".sr-hotel__name::text\").get(),\n",
    "                   \"url\": \"https://www.booking.com\" + h.css(\".hotel_name_link\").attrib[\"href\"],\n",
    "                   \"coords\": h.css(\".sr_card_address_line a\").attrib[\"data-coords\"],\n",
    "                   \"score\": h.css(\".bui-review-score__badge::text\").get(),\n",
    "                   \"description\": h.css(\".hotel_desc::text\").get()}\n",
    "        \n",
    "        try:\n",
    "            next_page = response.css(\"a.paging-next\").attrib[\"href\"]\n",
    "        except KeyError:\n",
    "            logging.info(\"No next page. Terminating crawling process.\")\n",
    "        else:\n",
    "            yield response.follow(next_page, callback = self.after_search)\n",
    "            \n",
    "# Here is the only line that changes!\n",
    "\n",
    "destination_name = top_destinations[4]\n",
    "\n",
    "filename = \"hotels_\" + destination_name.replace(\" \", \"-\") + \".json\"\n",
    "\n",
    "if filename in os.listdir(\"hotels_data/\"):\n",
    "    os.remove(\"hotels_data/\" + filename)\n",
    "\n",
    "process = CrawlerProcess(settings = {\"USER_AGENT\": \"Chrome/84.0 (compatible; MSIE 7.0; Windows NT 5.1)\",\n",
    "                                     \"LOG_LEVEL\": logging.WARNING,\n",
    "                                     \"FEEDS\": {\"hotels_data/\" + filename: {\"format\": \"json\"}}})\n",
    "\n",
    "process.crawl(Hotels)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are! Check in your freshly created `hotels_data` folder, you should have **a JSON file for each of our top 5 cities**.\n",
    "\n",
    "It is now time to **explore these hotels lists** and combine them with the table generated in the previous notebook. That will be the purpose of **Part 3**!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "01-Plan_your_trip_with_Kayak_solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
